<!DOCTYPE html>
<html lang="en">
<head hexo-theme='https://github.com/volantis-x/hexo-theme-volantis/tree/4.1.1'>
  <meta charset="utf-8">
  <!-- SEO相关 -->
  
    
  
  <!-- 渲染优化 -->
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://cdn.jsdelivr.net'>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- 页面元数据 -->
  
  <title>【教程】Tensorflow vs PyTorch —— 自动求导 - Hongtao&#39;s Blog</title>
  
    <meta name="keywords" content="tutorial,tensorflow,pytorch">
  

  

  <!-- feed -->
  

  <!-- import meta -->
  

  <!-- link -->
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14/css/all.min.css">

  
  

  

  

  

  

  <!-- import link -->
  

  
  
    
<link rel="stylesheet" href="/css/style.css">

  

  <script id="loadcss"></script>

</head>

<body>
  

<header id="l_header" class="l_header auto shadow blur show" style='opacity: 0' >
  <div class='container'>
  <div id='wrapper'>
    <div class='nav-sub'>
      <p class="title"></p>
      <ul class='switcher nav-list-h m-phone' id="pjax-header-nav-list">
        <li><a id="s-comment" class="fas fa-comments fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
          <li><a id="s-toc" class="s-toc fas fa-list fa-fw" target="_self" href='javascript:void(0)'></a></li>
        
      </ul>
    </div>
		<div class="nav-main">
      
        
        <a class="title flat-box" target="_self" href='/'>
          
            <img no-lazy class='logo' src='https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/Logo-NavBar@3x.png'/>
          
          
          
        </a>
      

			<div class='menu navigation'>
				<ul class='nav-list-h m-pc'>
          
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
          
            
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
          
				</ul>
			</div>

      <div class="m_search">
        <form name="searchform" class="form u-search-form">
          <i class="icon fas fa-search fa-fw"></i>
          <input type="text" class="input u-search-input" placeholder="Search..." />
        </form>
      </div>

			<ul class='switcher nav-list-h m-phone'>
				
					<li><a class="s-search fas fa-search fa-fw" target="_self" href='javascript:void(0)'></a></li>
				
				<li>
          <a class="s-menu fas fa-bars fa-fw" target="_self" href='javascript:void(0)'></a>
          <ul class="menu-phone list-v navigation white-box">
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/
                  
                  
                  
                    id="home"
                  >
                  <i class='fas fa-rss fa-fw'></i>博客
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/categories/
                  
                  
                  
                    id="categories"
                  >
                  <i class='fas fa-folder-open fa-fw'></i>分类
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/tags/
                  
                  
                  
                    id="tags"
                  >
                  <i class='fas fa-tags fa-fw'></i>标签
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/archives/
                  
                  
                  
                    id="archives"
                  >
                  <i class='fas fa-archive fa-fw'></i>归档
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/friends/
                  
                  
                  
                    id="friends"
                  >
                  <i class='fas fa-link fa-fw'></i>友链
                </a>
                
              </li>
            
          
            
              
            
              <li>
                <a class="menuitem flat-box faa-parent animated-hover" href=/about/
                  
                  
                  
                    id="about"
                  >
                  <i class='fas fa-info-circle fa-fw'></i>关于
                </a>
                
              </li>
            
          
            
          </ul>
        </li>
			</ul>
		</div>
	</div>
  </div>
</header>

  <div id="l_body">
    <div id="l_cover">
  
    
        <div id="full" class='cover-wrapper post dock' style="display: none;">
          
            <div class='cover-bg lazyload placeholder' data-bg="https://source.unsplash.com/random"></div>
          
          <div class='cover-body'>
  <div class='top'>
    
    
      <p class="title">Hongtao's Blog</p>
    
    
  </div>
  <div class='bottom'>
    <div class='menu navigation'>
      <div class='list-h'>
        
      </div>
    </div>
  </div>
</div>

          <div id="scroll-down" style="display: none;"><i class="fa fa-chevron-down scroll-down-effects"></i></div>
        </div>
    
  
  </div>

    <div id='safearea'>
      <div class='body-wrapper' id="pjax-container">
        

<div class='l_main'>
  <article class="article post white-box reveal md shadow article-type-post" id="post" itemscope itemprop="blogPost">
  


  
  <div class="article-meta" id="top">
    
    
    
      <h1 class="title">
        【教程】Tensorflow vs PyTorch —— 自动求导
      </h1>
      <div class='new-meta-box'>
        
          
            
<div class='new-meta-item author'>
  <a class='author' href="/" rel="nofollow">
    <img no-lazy src="">
    <p>请设置文章作者</p>
  </a>
</div>

          
        
          
            

          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>
    <p>发布于：Feb 17, 2020</p>
  </a>
</div>

          
        
          
            
  <div class="new-meta-item browse leancloud">
    <a class='notlink'>
      
      <div id="lc-pv" data-title="【教程】Tensorflow vs PyTorch —— 自动求导" data-path="/2020/02/17/%E3%80%90%E6%95%99%E7%A8%8B%E3%80%91Tensorflow%20vs%20PyTorch%20%E2%80%94%E2%80%94%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/">
        <i class="fas fa-eye fa-fw" aria-hidden="true"></i>
        <span id='number'><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span>
        次浏览
      </div>
    </a>
  </div>


          
        
      </div>
    
  </div>


  
  <p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbzw6xfd26j30rs0ij42b.jpg" class="lazyload" data-srcset="https://tva1.sinaimg.cn/large/0082zybpgy1gbzw6xfd26j30rs0ij42b.jpg" srcset="data:image/png;base64,666" alt="img"></p>
<p><em>image from unsplash.com by <a target="_blank" rel="noopener" href="https://unsplash.com/@johnwestrock">@johnwestrock</a></em></p>
<p>在深度学习中，网络参数的优化是通过 <em>后向传播</em> 实现的，而优化参数的最基本方法就是 <em>梯度下降</em> 法。使用该方法首先就要求参数对损失函数的的梯度。<em>梯度</em> 可以简单理解为参数对于损失函数的导数(实际上是偏导数)。</p>
<p>Tensorflow 和 PyTorch 之所以强大，是因为其自动求导的功能和自动优化能力，本文就来对比介绍 Tensorflow 和 PyTorch的自动求导和自动优化功能。</p>
<h3 id="1-多项式求导"><a href="#1-多项式求导" class="headerlink" title="1. 多项式求导"></a>1. 多项式求导</h3><p>为了方便理解我们这里定义一个非常简单的多项式函数：<br>$$<br>y = x^2 + x^3 + 5<br>$$<br>x 对 y 的导数为<br>$$<br>dy/dx = 2x + 3x^2<br>$$<br>当 x = 2 时 dy/dx = 2 * 2 + 3 *4 = 16。</p>
<p>下面我们用 Tensorflow 和 PyToch 验证一下，需要注意以下几点：</p>
<blockquote>
<ol>
<li>Tensorflow 中只有 Variables 才能被自动求导，如果是 constant 的张量需要 被<code>tape.watch()</code>。</li>
<li>在 PyTorch 中张量需要开启 <code>requires_grad = True</code> 其计算才会被记录。</li>
<li>Tensorflow 中使用 <code>with tf.GradientTape() as tape:</code> 包裹并记录计算过程以便求导，PyTorch 会自动记录变量的计算过程。</li>
<li>Tensorflow 中 <code>tape.gradient(y, [x])</code> 返回的是一个list，与之相对应 <code>torch.autograd.grad(y, [x])</code> 返回的是元组(tuple).</li>
<li>PyTorch 还可以使用 y.backward() 对所有参数自动求导，然后用<code>.grad</code> 获取相应参数的导数</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span> + x**<span class="number">3</span> + <span class="number">5</span></span><br><span class="line"><span class="comment"># df(x)/dx = 2x + 3x**2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------Tensorflow -----------------------------</span></span><br><span class="line">x = tf.Variable([<span class="number">2.</span>])</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    y = func(x)</span><br><span class="line">    </span><br><span class="line">grad = tape.gradient(y, [x])</span><br><span class="line">print(grad[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># ------------------------PyTorch ---------------------------------</span></span><br><span class="line">x = torch.tensor([<span class="number">2.</span>], requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = func(x)</span><br><span class="line"><span class="comment">#----method 1------</span></span><br><span class="line">grad = torch.autograd.grad(y, [x])</span><br><span class="line">print(grad[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#----method 2------</span></span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<h3 id="2-MSE-损失函数求导"><a href="#2-MSE-损失函数求导" class="headerlink" title="2. MSE 损失函数求导"></a>2. MSE 损失函数求导</h3><p>MSE 既 Mean Square Root (均方差) 损失函数在机器学习中用得非常广泛的一个损失函数。其物理意义就是计算预测值与真实值之间的<em>“距离”</em>并取其平均数。优化模型的过程就是<em>”缩短“</em> 这个距离的过程。</p>
<p>这里我们假设了一个场景：</p>
<blockquote>
<p>数据bach size 为3，输入特征维度为4，size：[3, 4], 输出 y 类别数量(depth)为2，size: [3]<br>线性变换 y = x*w + b, 待优化参数size  w:[4,2] b:[2]</p>
</blockquote>
<p>我们分别用 Tensorflow 和 PyTorch 计算MSE损失函数的梯度，代码如下：</p>
<blockquote>
<p>注意以下几点：</p>
<ol>
<li>线性变换 x@w + b 后得到的是 logits，既每个类别的 <em>”得分“</em>，需通过 softmax 函数转化属于每个类别的*”概率”* probs</li>
<li>计算 probs 和 真实y 的MSE 需要将真实的 y ”one-hot“ 编码，既正确类别的概率为1，其他类别为0。 Tensorflow 自带 one-hot 编码，PyTorch 需要手动实现。</li>
<li>Softmax 和 MSE的实现方式在 Tensorflow 和 PyTorch 中实现的方式有多种，有方程的方式，有对象的方式，由于 Tensorflow 2.0 与 Keras 的融合，在 Keras 中也有相应的方式。</li>
<li>tf.losses.MSE 返回是一个一维的张量，需要用 <code>reduce_mean</code> 计算出一个标量(Scalar)。 </li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example: [3,4] linear conversion -&gt;[3,2]</span></span><br><span class="line"><span class="comment">#  y = x@w + b  x:[3,4] w:[4,2] b:[2], y:[3]</span></span><br><span class="line"><span class="comment">#  y one-hot depth = 2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------Tensorflow -----------------------------</span></span><br><span class="line">x = tf.random.uniform([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">w = tf.random.uniform([<span class="number">4</span>,<span class="number">2</span>])</span><br><span class="line">b = tf.zeros([<span class="number">2</span>])</span><br><span class="line">y = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    <span class="comment"># if the tensors are not variables</span></span><br><span class="line">    tape.watch([w,b])</span><br><span class="line">    </span><br><span class="line">    logits = x @ w + b</span><br><span class="line">    probs = tf.nn.softmax(logits)</span><br><span class="line">    </span><br><span class="line">    y_true = tf.one_hot(y, depth=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    losses = tf.losses.MSE(y_true,probs)</span><br><span class="line">    loss = tf.reduce_mean(losses)</span><br><span class="line">    </span><br><span class="line">grads = tape.gradient(loss, [w,b])</span><br><span class="line"></span><br><span class="line">grads_w = grads[<span class="number">0</span>]</span><br><span class="line">grads_b = grads[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">print(loss)</span><br><span class="line">print(grads[<span class="number">0</span>])</span><br><span class="line">print(grads[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------PyTorch ---------------------------------</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span>(<span class="params">label, depth</span>):</span></span><br><span class="line">    out = torch.zeros(label.size(<span class="number">0</span>), depth)</span><br><span class="line">    idx = torch.LongTensor(label).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    out.scatter_(dim=<span class="number">1</span>, index=idx, value=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">x = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">w = torch.rand([<span class="number">4</span>,<span class="number">2</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros([<span class="number">2</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.LongTensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># if &quot;requires_grad=Flase&quot;</span></span><br><span class="line"><span class="comment"># w.requires_grad_()</span></span><br><span class="line"><span class="comment"># b.requires_grad_()</span></span><br><span class="line"></span><br><span class="line">logits = x @ w +b</span><br><span class="line">probs = F.softmax(logits, dim = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y_true = one_hot(y, depth=<span class="number">2</span>)</span><br><span class="line">loss = F.mse_loss(y_true, probs)</span><br><span class="line"></span><br><span class="line">grads = torch.autograd.grad(loss, [w, b])</span><br><span class="line"></span><br><span class="line">grads_w = grads[<span class="number">0</span>]</span><br><span class="line">grads_b = grads[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(loss)</span><br><span class="line">print(grads_w)</span><br><span class="line">print(grads_b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternative way:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss.backward()</span></span><br><span class="line"><span class="comment"># print(w.grad)</span></span><br><span class="line"><span class="comment"># print(b.grad)</span></span><br></pre></td></tr></table></figure>

<h3 id="3-链式法则"><a href="#3-链式法则" class="headerlink" title="3. 链式法则"></a>3. 链式法则</h3><p>给一个不严谨的定义，链式法则是指在多层嵌套的函数中，内层参数对于外层函数的偏微分可以像链条一样，从外向内一步一步地求出。这使得对于无论有多深的神经网络，其每一层的参数对于损失函数的偏微分都可以通过链式法则求得。</p>
<p>这里我们简单举一个两次线性变换的例子</p>
<blockquote>
<p>y1 = x1 * w1 + b1<br>y2 = y1 * w2 + b2</p>
</blockquote>
<p>我们用 Tensorflow 和 PyTorch 验证一下 dw2/dy1 是否等于 dw2/dy2 * dy2/y1。 答案当然是肯定的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------Tensorflow -----------------------------</span></span><br><span class="line">x1 = tf.random.uniform([<span class="number">1</span>])</span><br><span class="line">w1 = tf.random.uniform([<span class="number">1</span>])</span><br><span class="line">b1 = tf.random.uniform([<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">w2 = tf.random.uniform([<span class="number">1</span>])</span><br><span class="line">b2 = tf.random.uniform([<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:</span><br><span class="line">    tape.watch([w1,b1,w2,b2])</span><br><span class="line">    </span><br><span class="line">    y1 = x1*w1 + b1</span><br><span class="line">    y2 = y1*w2 + b2</span><br><span class="line">    </span><br><span class="line">[dy1_dw1] = tape.gradient(y1, [w1])</span><br><span class="line">[dy2_dy1] = tape.gradient(y2, [y1])</span><br><span class="line">     </span><br><span class="line">[dy2_dw1] = tape.gradient(y2, [w1])</span><br><span class="line"></span><br><span class="line">print(dy2_dw1 == dy2_dy1 * dy1_dw1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------PyTorch ---------------------------------</span></span><br><span class="line">x1 = torch.rand(<span class="number">1</span>)</span><br><span class="line">w1 = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w2 = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.rand(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y1 = x1*w1 + b1</span><br><span class="line">y2 = y1*w2 + b2</span><br><span class="line"></span><br><span class="line">(dy1_dw1,) = torch.autograd.grad(y1, w1, retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">(dy2_dy1,) = torch.autograd.grad(y2, y1, retain_graph=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(dy2_dw1,) = torch.autograd.grad(y2, w1)</span><br><span class="line"></span><br><span class="line">print(dy2_dy1 * dy1_dw1 == dy2_dw1)</span><br></pre></td></tr></table></figure>

<h3 id="4-参数优化"><a href="#4-参数优化" class="headerlink" title="4. 参数优化"></a>4. 参数优化</h3><p>得到参数的梯度之后，我们可以使用 <em>梯度下降</em> 的方法对参数进行优化。这里我们使用了 Himmelblau 函数。这个函数一共有 4 个零点，</p>
<blockquote>
<p>[3, 2], [-2.805118, 3.131312], [-3.779310, -3.283186], [3.583328, -1.848126]</p>
</blockquote>
<p>公式和 3d 图如下：<br>$$<br>f(x, y)=\left(x^{2}+y-11\right)^{2}+\left(x+y^{2}-7\right)^{2}<br>$$<br><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbzv46bdugj308c06975f.jpg" class="lazyload" data-srcset="https://tva1.sinaimg.cn/large/0082zybpgy1gbzv46bdugj308c06975f.jpg" srcset="data:image/png;base64,666" alt="img"></p>
<p><em>image from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Himmelblau%27s_function">wikipedia</a></em></p>
<h4 id="4-1-手动梯度下降"><a href="#4-1-手动梯度下降" class="headerlink" title="4.1 手动梯度下降"></a>4.1 手动梯度下降</h4><p>公式的方程代码里就略过了，我们在 Tensorflow 和 PyTorch 中分别用梯度下降的方法寻找零点。</p>
<blockquote>
<p>注意以下几点：</p>
<ol>
<li>这里优化的参数是 x，不能直接使用 x -= lr * grad 来更新参数 ,  Tensorflow 使用<code>x.assign_sub(lr * grads[0])</code> PyTorch 使用 <code>x.data.sub_(lr * grads[0])</code></li>
<li>这里 x 的初始位置为 [0, 0], 优化后找到 [3,2] 这个点，改变初始位置会改变找到的地方。</li>
<li>由于 learning rate (学习速率) 被固定在了 0.001, 所以手动梯度下降永远也到不了真正的 <em>零点</em>， 需要动态改变学习速率或者采用其他优化方法。</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------Tensorflow -----------------------------</span></span><br><span class="line">x = tf.Variable([<span class="number">0.</span>,<span class="number">0.</span>])</span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">30000</span>): </span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred = himmelblau(x)</span><br><span class="line">        </span><br><span class="line">    grads = tape.gradient(pred, [x])</span><br><span class="line">    x.assign_sub(lr * grads[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(step % <span class="number">2000</span> == <span class="number">0</span>):</span><br><span class="line">        print(<span class="string">&#x27;step &#123;&#125;: x = &#123;&#125;, pred = &#123;&#125;&#x27;</span></span><br><span class="line">             .format(step, x.numpy(), pred.numpy()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------PyTorch ---------------------------------</span></span><br><span class="line">x = torch.tensor([<span class="number">0.</span>,<span class="number">0.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">30000</span>):</span><br><span class="line">    </span><br><span class="line">    pred = himmelblau(x)</span><br><span class="line">    </span><br><span class="line">    grads = torch.autograd.grad(pred, [x])</span><br><span class="line">    x.data.sub_(lr * grads[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(step % <span class="number">2000</span> == <span class="number">0</span>):</span><br><span class="line">        print(<span class="string">&#x27;step &#123;&#125;: x = &#123;&#125;, pred = &#123;&#125;&#x27;</span></span><br><span class="line">             .format(step, x.tolist(), pred.item()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="4-2-自动优化"><a href="#4-2-自动优化" class="headerlink" title="4.2 自动优化"></a>4.2 自动优化</h4><p>在实际的深度学习训练中，我们完全没有必要自己手动写梯度下降的代码，Tensorflow 和 PyTorch 自带了包括梯度下降的各种优化器。</p>
<blockquote>
<ol>
<li>在 Tensorflow 中，我们首先定义一个优化器 <code>optimizer</code>, 在训练过程中使用 <code>optimizer.apply_gradients(zip(grads, [x]))</code> 既可完成训练。</li>
<li>在 PyTorch 中，也需要首先定义一个  <code>optimizer</code> 并指定优化参数， 在训练中，zero_grad() backward(), step() 三步即可完成训练。</li>
<li>SGD 既随机梯度下降，将其变换成 Adam 就会解决SGD找不到真正 零点的问题，感兴趣的读者不妨试试。</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------Tensorflow -----------------------------</span></span><br><span class="line">x = tf.Variable([<span class="number">0.</span>,<span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">optimizer = tf.optimizers.SGD(lr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">30000</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        pred = himmelblau(x)</span><br><span class="line">    </span><br><span class="line">    grads = tape.gradient(pred, [x])</span><br><span class="line">    </span><br><span class="line">    optimizer.apply_gradients(grads_and_vars = zip(grads, [x]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(step % <span class="number">2000</span> == <span class="number">0</span>):</span><br><span class="line">        print(<span class="string">&#x27;step &#123;&#125;: x = &#123;&#125;, pred = &#123;&#125;&#x27;</span></span><br><span class="line">             .format(step, x.numpy(), pred.numpy()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------PyTorch ---------------------------------</span></span><br><span class="line">x = torch.tensor([<span class="number">0.</span>,<span class="number">0.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">optimizer = torch.optim.SGD([x],lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">30000</span>):</span><br><span class="line">    </span><br><span class="line">    pred = himmelblau(x)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    pred.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(step % <span class="number">2000</span> == <span class="number">0</span>):</span><br><span class="line">        print(<span class="string">&#x27;step &#123;&#125;: x = &#123;&#125;, pred = &#123;&#125;&#x27;</span></span><br><span class="line">             .format(step, x.tolist(), pred.item()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


  
  
    
    <div class='footer'>
      
      
      
        <div class='copyright'>
          <blockquote>
            
              
                <p>博客内容优先发布于本人微信公众号“Tensorflow机器学习”，欢迎关注，转载请注明出处</p>

              
            
          </blockquote>
        </div>
      
      
    </div>
  
  
    


  <div class='article-meta' id="bottom">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2020-10-14T20:48:02+01:00">
  <a class='notlink'>
    <i class="fas fa-edit fa-fw" aria-hidden="true"></i>
    <p>更新于：Oct 14, 2020</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/tutorial/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>tutorial</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/tensorflow/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>tensorflow</p></a></div> <div class="new-meta-item meta-tags"><a class="tag" href="/tags/pytorch/" rel="nofollow"><i class="fas fa-hashtag fa-fw" aria-hidden="true"></i><p>pytorch</p></a></div>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://example.com/2020/02/17/%E3%80%90%E6%95%99%E7%A8%8B%E3%80%91Tensorflow%20vs%20PyTorch%20%E2%80%94%E2%80%94%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/&title=【教程】Tensorflow vs PyTorch —— 自动求导 - Hongtao's Blog&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qq.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qq.png" srcset="data:image/png;base64,666">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://example.com/2020/02/17/%E3%80%90%E6%95%99%E7%A8%8B%E3%80%91Tensorflow%20vs%20PyTorch%20%E2%80%94%E2%80%94%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/&title=【教程】Tensorflow vs PyTorch —— 自动求导 - Hongtao's Blog&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qzone.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/qzone.png" srcset="data:image/png;base64,666">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="" rel="external nofollow noopener noreferrer noopener"
          
          target="_blank" href="http://service.weibo.com/share/share.php?url=http://example.com/2020/02/17/%E3%80%90%E6%95%99%E7%A8%8B%E3%80%91Tensorflow%20vs%20PyTorch%20%E2%80%94%E2%80%94%20%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/&title=【教程】Tensorflow vs PyTorch —— 自动求导 - Hongtao's Blog&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/weibo.png" class="lazyload" data-srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/logo/128/weibo.png" srcset="data:image/png;base64,666">
          
        </a>
      
    
      
    
      
    
  </div>
</div>



        
      
    </div>
  </div>


  
  

  
    <div class="prev-next">
      
        <a class='prev' href='/2020/02/24/%E3%80%90%E6%95%99%E7%A8%8B%E3%80%91Tensorflow%20vs%20PyTorch%20%E2%80%94%E2%80%94%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%AE%AD%E7%BB%83/'>
          <p class='title'><i class="fas fa-chevron-left" aria-hidden="true"></i>【教程】Tensorflow vs PyTorch —— 神经网络的搭建与训练</p>
          <p class='content'>
image from unsplash.com by @wolfgang_hasselmann
上一篇文章，我们用 Tensorflow 和 PyTorch 分别完成了函数自动求导以及参数手动...</p>
        </a>
      
      
        <a class='next' href='/2020/02/15/%E3%80%90%E6%95%99%E7%A8%8B%E3%80%91Tensorflow%20vs%20PyTorch%20%E2%80%94%E2%80%94%20%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97/'>
          <p class='title'>【教程】Tensorflow vs PyTorch —— 数学运算<i class="fas fa-chevron-right" aria-hidden="true"></i></p>
          <p class='content'>掌握 Tensorflow 和 PyTorch 的基本数学运算操作，对后续机器学习和深度学习的学习十分重要。当然两者在这方面也十分相似。
1. 基本数学运算基本数学运算即“加减乘除“，Tenso...</p>
        </a>
      
    </div>
  
</article>


  

  <article class="post white-box reveal shadow" id="comments">
    <p ct><i class='fas fa-comments'></i> 评论</p>
    
    <div id="valine_container" class="valine_thread">
  <i class="fas fa-cog fa-spin fa-fw fa-2x"></i>
</div>

  </article>






</div>
<aside class='l_side'>
  
  
    
    



  <section class="widget toc-wrapper shadow desktop mobile" id="toc-div" >
    
  <header>
    
      <i class="fas fa-list fa-fw" aria-hidden="true"></i><span class='name'>本文目录</span>
    
  </header>


    <div class='content'>
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%B1%82%E5%AF%BC"><span class="toc-text">1. 多项式求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-MSE-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC"><span class="toc-text">2. MSE 损失函数求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99"><span class="toc-text">3. 链式法则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96"><span class="toc-text">4. 参数优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E6%89%8B%E5%8A%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">4.1 手动梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-%E8%87%AA%E5%8A%A8%E4%BC%98%E5%8C%96"><span class="toc-text">4.2 自动优化</span></a></li></ol></li></ol>
    </div>
  </section>


  


</aside>



        
        
          <!--此文件用来存放一些不方便取值的变量-->
<!--思路大概是将值藏到重加载的区域内-->

<script>
  window.pdata={}
  pdata.ispage=true;
  pdata.postTitle="【教程】Tensorflow vs PyTorch —— 自动求导";
  pdata.commentPath="";
  pdata.commentPlaceholder="";

  var l_header=document.getElementById("l_header");
  
  l_header.classList.add("show");
  
</script>

        
      </div>
      
  
  <footer class="footer clearfix">
    <br><br>
    
      
        <div class="aplayer-container">
          


        </div>
      
    
      
        <br>
        <div class="social-wrapper">
          
            
          
            
          
            
          
        </div>
      
    
      
        <div><p>Blog content follows the <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License</a></p>
</div>
      
    
      
        
          <div><p><span id="lc-sv">本站总访问量为 <span id='number'><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span> 次</span> <span id="lc-uv">访客数为 <span id='number'><i class="fas fa-circle-notch fa-spin fa-fw" aria-hidden="true"></i></span> 人</span></p>
</div>
        
      
    
      
        Use
        <a href="https://github.com/volantis-x/hexo-theme-volantis/tree/4.1.1" target="_blank" class="codename">Volantis</a>
        as theme
      
    
      
        <div class='copyright'>
        <p><a href="/">Copyright © 2017-2020</a></p>

        </div>
      
    
  </footer>


      <a id="s-top" class="fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
    </div>
  </div>
  <div>
    <script>
window.volantis={};
/********************脚本懒加载函数********************************/
function loadScript(src, cb) {
var HEAD = document.getElementsByTagName('head')[0] || document.documentElement;
var script = document.createElement('script');
script.setAttribute('type','text/javascript');
if (cb) script.onload = cb;
script.setAttribute('src', src);
HEAD.appendChild(script);
}
//https://github.com/filamentgroup/loadCSS
var loadCSS = function( href, before, media, attributes ){
	var doc = window.document;
	var ss = doc.createElement( "link" );
	var ref;
	if( before ){
		ref = before;
	}
	else {
		var refs = ( doc.body || doc.getElementsByTagName( "head" )[ 0 ] ).childNodes;
		ref = refs[ refs.length - 1];
	}
	var sheets = doc.styleSheets;
	if( attributes ){
		for( var attributeName in attributes ){
			if( attributes.hasOwnProperty( attributeName ) ){
				ss.setAttribute( attributeName, attributes[attributeName] );
			}
		}
	}
	ss.rel = "stylesheet";
	ss.href = href;
	ss.media = "only x";
	function ready( cb ){
		if( doc.body ){
			return cb();
		}
		setTimeout(function(){
			ready( cb );
		});
	}
	ready( function(){
		ref.parentNode.insertBefore( ss, ( before ? ref : ref.nextSibling ) );
	});
	var onloadcssdefined = function( cb ){
		var resolvedHref = ss.href;
		var i = sheets.length;
		while( i-- ){
			if( sheets[ i ].href === resolvedHref ){
				return cb();
			}
		}
		setTimeout(function() {
			onloadcssdefined( cb );
		});
	};
	function loadCB(){
		if( ss.addEventListener ){
			ss.removeEventListener( "load", loadCB );
		}
		ss.media = media || "all";
	}
	if( ss.addEventListener ){
		ss.addEventListener( "load", loadCB);
	}
	ss.onloadcssdefined = onloadcssdefined;
	onloadcssdefined( loadCB );
	return ss;
};
</script>
<!-- required -->

<script src="https://cdn.jsdelivr.net/npm/jquery@3.5/dist/jquery.min.js"></script>

<script>
  function pjax_fancybox() {
    $(".md .gallery").find("img").each(function () { //渲染 fancybox
      var element = document.createElement("a"); // a 标签
      $(element).attr("class", "fancybox");
      $(element).attr("pjax-fancybox", "");  // 过滤 pjax
      $(element).attr("href", $(this).attr("src"));
      if ($(this).attr("data-original")) {
        $(element).attr("href", $(this).attr("data-original"));
      }
      $(element).attr("data-fancybox", "images");
      var caption = "";   // 描述信息
      if ($(this).attr('alt')) {  // 判断当前页面是否存在描述信息
        $(element).attr('data-caption', $(this).attr('alt'));
        caption = $(this).attr('alt');
      }
      var div = document.createElement("div");
      $(div).addClass("fancybox");
      $(this).wrap(div); // 最外层套 div ，其实主要作用还是 class 样式
      var span = document.createElement("span");
      $(span).addClass("image-caption");
      $(span).text(caption); // 加描述
      $(this).after(span);  // 再套一层描述
      $(this).wrap(element);  // 最后套 a 标签
    })
    $(".md .gallery").find("img").fancybox({
      selector: '[data-fancybox="images"]',
      hash: false,
      loop: false,
      closeClick: true,
      helpers: {
        overlay: {closeClick: true}
      },
      buttons: [
        "zoom",
        "close"
      ]
    });
  };
  function SCload_fancybox() {
    if ($(".md .gallery").find("img").length == 0) return;
    loadCSS("https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css", document.getElementById("loadcss"));
    setTimeout(function() {
      loadScript('https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js', pjax_fancybox)
    }, 1);
  };
  $(function () {
    SCload_fancybox();
  });
</script>


<!-- internal -->







  <script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazyload",
    threshold: 0
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    lazyLoadInstance.update();
  });
  document.addEventListener('pjax:complete', function () {
    lazyLoadInstance.update();
  });
</script>




  
  
    <script>
      window.FPConfig = {
        delay: 0,
        ignoreKeywords: [],
        maxRPS: 5,
        hoverDelay: 25
      };
    </script>
    <script defer src="https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"></script>
  










  
  
<script src="/js/valine.js"></script>


<script>
  var GUEST_INFO = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link'.split(',').filter(function (item) {
    return GUEST_INFO.indexOf(item) > -1
  });
  var REQUIRED_FIELDS = ['nick', 'mail', 'link'];
  var requiredFields = 'nick,mail'.split(',').filter(function (item) {
    return REQUIRED_FIELDS.indexOf(item) > -1
  });

  function emoji(path, idx, ext) {
    return path + "/" + path + "-" + idx + "." + ext;
  }

  var emojiMaps = {};
  for (var i = 1; i <= 54; i++) {
    emojiMaps['tieba-' + i] = emoji('tieba', i, 'png');
  }
  for (var i = 1; i <= 101; i++) {
    emojiMaps['qq-' + i] = emoji('qq', i, 'gif');
  }
  for (var i = 1; i <= 116; i++) {
    emojiMaps['aru-' + i] = emoji('aru', i, 'gif');
  }
  for (var i = 1; i <= 125; i++) {
    emojiMaps['twemoji-' + i] = emoji('twemoji', i, 'png');
  }
  for (var i = 1; i <= 4; i++) {
    emojiMaps['weibo-' + i] = emoji('weibo', i, 'png');
  }

  function pjax_valine() {
    if(!document.querySelectorAll("#valine_container")[0])return;

    let pagePlaceholder = pdata.commentPlaceholder || "快来评论吧~";

    let path = pdata.commentPath;
    if (path.length == 0) {
      let defaultPath = '';
      path = defaultPath || decodeURI(window.location.pathname);
    }

    var valine = new Valine();
    valine.init({
      el: '#valine_container',
      meta: meta,
      placeholder: pagePlaceholder,
      path: path,
      appId: "",
      appKey: "",
      pageSize: '10',
      avatar: 'robohash',
      lang: 'zh-cn',
      visitor: 'true',
      highlight: 'true',
      mathJax: 'false',
      enableQQ: 'true',
      recordIP: 'false',
      requiredFields: requiredFields,
      emojiCDN: 'https://cdn.jsdelivr.net/gh/volantis-x/cdn-emoji/valine/',
      emojiMaps: emojiMaps
    })
  }

  $(function () {
    pjax_valine();
  });
</script>





  
<script src="/js/app.js"></script>




  
    
<script src="/js/search.js"></script>

  


<!-- optional -->

  <script>
const SearchServiceimagePath="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@master/img/";
const ROOT =  ("/" || "/").endsWith('/') ? ("/" || "/") : ("//" || "/" );
(function ($) {
  
    customSearch = new HexoSearch({
      imagePath: SearchServiceimagePath
    });
  
})(jQuery);

</script>











  <script defer>

  const LCCounter = {
    app_id: 'u9j57bwJod4EDmXWdxrwuqQT-MdYXbMMI',
    app_key: 'jfHtEKVE24j0IVCGHbvuFClp',
    custom_api_server: '',

    // 查询存储的记录
    getRecord(Counter, url, title) {
      return new Promise(function (resolve, reject) {
        Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({url})))
          .then(resp => resp.json())
          .then(({results, code, error}) => {
            if (code === 401) {
              throw error;
            }
            if (results && results.length > 0) {
              var record = results[0];
              resolve(record);
            } else {
              Counter('post', '/classes/Counter', {url, title: title, times: 0})
                .then(resp => resp.json())
                .then((record, error) => {
                  if (error) {
                    throw error;
                  }
                  resolve(record);
                }).catch(error => {
                console.error('Failed to create', error);
                reject(error);
              });
            }
          }).catch((error) => {
          console.error('LeanCloud Counter Error:', error);
          reject(error);
        });
      })
    },

    // 发起自增请求
    increment(Counter, incrArr) {
      return new Promise(function (resolve, reject) {
        Counter('post', '/batch', {
          "requests": incrArr
        }).then((res) => {
          res = res.json();
          if (res.error) {
            throw res.error;
          }
          resolve(res);
        }).catch((error) => {
          console.error('Failed to save visitor count', error);
          reject(error);
        });
      });
    },

    // 构建自增请求体
    buildIncrement(objectId) {
      return {
        "method": "PUT",
        "path": `/1.1/classes/Counter/${ objectId }`,
        "body": {
          "times": {
            '__op': 'Increment',
            'amount': 1
          }
        }
      }
    },

    // 校验是否为有效的 UV
    validUV() {
      var key = 'LeanCloudUVTimestamp';
      var flag = localStorage.getItem(key);
      if (flag) {
        // 距离标记小于 24 小时则不计为 UV
        if (new Date().getTime() - parseInt(flag) <= 86400000) {
          return false;
        }
      }
      localStorage.setItem(key, new Date().getTime().toString());
      return true;
    },

    addCount(Counter) {
      var enableIncr = '' === 'true' && window.location.hostname !== 'localhost';
      enableIncr = true;
      var getterArr = [];
      var incrArr = [];
      // 请求 PV 并自增
      var pvCtn = document.querySelector('#lc-sv');
      if (pvCtn || enableIncr) {
        var pvGetter = this.getRecord(Counter, 'http://example.com' + '/#lc-sv', 'Visits').then((record) => {
          incrArr.push(this.buildIncrement(record.objectId))
          var eles = document.querySelectorAll('#lc-sv #number');
          if (eles.length > 0) {
            eles.forEach((el,index,array)=>{
              el.innerText = record.times + 1;
              if (pvCtn) {
                pvCtn.style.display = 'inline';
              }
            })
          }
        });
        getterArr.push(pvGetter);
      }

      // 请求 UV 并自增
      var uvCtn = document.querySelector('#lc-uv');
      if (uvCtn || enableIncr) {
        var uvGetter = this.getRecord(Counter, 'http://example.com' + '/#lc-uv', 'Visitors').then((record) => {
          var vuv = this.validUV();
          vuv && incrArr.push(this.buildIncrement(record.objectId))
          var eles = document.querySelectorAll('#lc-uv #number');
          if (eles.length > 0) {
            eles.forEach((el,index,array)=>{
              el.innerText = record.times + (vuv ? 1 : 0);
              if (uvCtn) {
                uvCtn.style.display = 'inline';
              }
            })
          }
        });
        getterArr.push(uvGetter);
      }

      // 请求文章的浏览数，如果是当前页面就自增
      var allPV = document.querySelectorAll('#lc-pv');
      if (allPV.length > 0 || enableIncr) {
        for (i = 0; i < allPV.length; i++) {
          let pv = allPV[i];
          let title = pv.getAttribute('data-title');
          var url = 'http://example.com' + pv.getAttribute('data-path');
          if (url) {
            var viewGetter = this.getRecord(Counter, url, title).then((record) => {
              // 是当前页面就自增
              let curPath = window.location.pathname;
              if (curPath.includes('index.html')) {
                curPath = curPath.substring(0, curPath.lastIndexOf('index.html'));
              }
              if (pv.getAttribute('data-path') == curPath) {
                incrArr.push(this.buildIncrement(record.objectId));
              }
              if (pv) {
                var ele = pv.querySelector('#lc-pv #number');
                if (ele) {
                  if (pv.getAttribute('data-path') == curPath) {
                    ele.innerText = (record.times || 0) + 1;
                  } else {
                    ele.innerText = record.times || 0;
                  }
                  pv.style.display = 'inline';
                }
              }
            });
            getterArr.push(viewGetter);
          }
        }
      }

      // 如果启动计数自增，批量发起自增请求
      if (enableIncr) {
        Promise.all(getterArr).then(() => {
          incrArr.length > 0 && this.increment(Counter, incrArr);
        })
      }

    },


    fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${ api_server }/1.1${ url }`, {
          method,
          headers: {
            'X-LC-Id': this.app_id,
            'X-LC-Key': this.app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      this.addCount(Counter);
    },

    refreshCounter() {
      var api_server = this.app_id.slice(-9) !== '-MdYXbMMI' ? this.custom_api_server : `https://${ this.app_id.slice(0, 8).toLowerCase() }.api.lncldglobal.com`;
      if (api_server) {
        this.fetchData(api_server);
      } else {
        fetch('https://app-router.leancloud.cn/2/route?appId=' + this.app_id)
          .then(resp => resp.json())
          .then(({api_server}) => {
            this.fetchData('https://' + api_server);
          });
      }
    }

  };

  LCCounter.refreshCounter();

  document.addEventListener('pjax:complete', function () {
    LCCounter.refreshCounter();
  });
</script>








<script>
function listennSidebarTOC() {
  const navItems = document.querySelectorAll(".toc li");
  if (!navItems.length) return;
  const sections = [...navItems].map((element) => {
    const link = element.querySelector(".toc-link");
    const target = document.getElementById(
      decodeURI(link.getAttribute("href")).replace("#", "")
    );
    link.addEventListener("click", (event) => {
      event.preventDefault();
      window.scrollTo({
		top: target.offsetTop + 100,
		
		behavior: "smooth"
		
	  });
    });
    return target;
  });

  function activateNavByIndex(target) {
    if (target.classList.contains("active-current")) return;

    document.querySelectorAll(".toc .active").forEach((element) => {
      element.classList.remove("active", "active-current");
    });
    target.classList.add("active", "active-current");
    let parent = target.parentNode;
    while (!parent.matches(".toc")) {
      if (parent.matches("li")) parent.classList.add("active");
      parent = parent.parentNode;
    }
  }

  function findIndex(entries) {
    let index = 0;
    let entry = entries[index];
    if (entry.boundingClientRect.top > 0) {
      index = sections.indexOf(entry.target);
      return index === 0 ? 0 : index - 1;
    }
    for (; index < entries.length; index++) {
      if (entries[index].boundingClientRect.top <= 0) {
        entry = entries[index];
      } else {
        return sections.indexOf(entry.target);
      }
    }
    return sections.indexOf(entry.target);
  }

  function createIntersectionObserver(marginTop) {
    marginTop = Math.floor(marginTop + 10000);
    let intersectionObserver = new IntersectionObserver(
      (entries, observe) => {
        let scrollHeight = document.documentElement.scrollHeight + 100;
        if (scrollHeight > marginTop) {
          observe.disconnect();
          createIntersectionObserver(scrollHeight);
          return;
        }
        let index = findIndex(entries);
        activateNavByIndex(navItems[index]);
      },
      {
        rootMargin: marginTop + "px 0px -100% 0px",
        threshold: 0,
      }
    );
    sections.forEach((element) => {
      element && intersectionObserver.observe(element);
    });
  }
  createIntersectionObserver(document.documentElement.scrollHeight);
}

document.addEventListener("DOMContentLoaded", listennSidebarTOC);
document.addEventListener("pjax:success", listennSidebarTOC);
</script>

<!-- more -->




    
      


<script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script>

<!-- 样式位于：source/css/_third-party/pjaxanimate.styl -->

<div class="pjax-animate">
  
    <script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>
    <div id="loading-bar-wrapper"><script>NProgress.configure({parent:"#loading-bar-wrapper",trickleSpeed: 100})</script></div>
    <script>
      window.ShowLoading = function() {
        NProgress.start();
      };
      window.HideLoading = function() {
        NProgress.done();
      }
    </script>
  
</div>

<script>
    var pjax;
    document.addEventListener('DOMContentLoaded', function () {
      pjax = new Pjax({
        elements: 'a[href]:not([href^="#"]):not([href="javascript:void(0)"]):not([pjax-fancybox])',
        selectors: [
          "title",
          "#l_cover",
          "#pjax-container",
          "#pjax-header-nav-list"
        ],
        cacheBust: false,   // url 地址追加时间戳，用以避免浏览器缓存
        timeout: 5000
      });
    });

    document.addEventListener('pjax:send', function (e) {
      //window.stop(); // 相当于点击了浏览器的停止按钮

      try {
        var currentUrl = window.location.pathname;
        var targetUrl = e.triggerElement.href;
        var banUrl = [""];
        if (banUrl[0] != "") {
          banUrl.forEach(item => {
            if(currentUrl.indexOf(item) != -1 || targetUrl.indexOf(item) != -1) {
              window.location.href = targetUrl;
            }
          });
        }
      } catch (error) {}

      window.subData = null; // 移除标题（用于一二级导航栏切换处）
      if (typeof $.fancybox != "undefined") {
        $.fancybox.close();    // 关闭弹窗
      }
      volantis.$switcher.removeClass('active'); // 关闭移动端激活的搜索框
      volantis.$header.removeClass('z_search-open'); // 关闭移动端激活的搜索框
      volantis.$wrapper.removeClass('sub'); // 跳转页面时关闭二级导航

      // 解绑事件 避免重复监听
      volantis.$topBtn.unbind('click');
      $('.menu a').unbind('click');
      $(window).unbind('resize');
      $(window).unbind('scroll');
      $(document).unbind('scroll');
      $(document).unbind('click');
      $('body').unbind('click');
      window.ShowLoading();
    });

    document.addEventListener('pjax:complete', function () {
      // 关于百度统计对 SPA 页面的处理：
      // 方案一：百度统计>管理>单页应用设置中，打开开启按钮即可对SPA进行统计。 https://tongji.baidu.com/web/help/article?id=324
      // 方案二：取消注释下列代码。 https://tongji.baidu.com/web/help/article?id=235
      // 

      // 关于谷歌统计对 SPA 页面的处理：
      // 当应用以动态方式加载内容并更新地址栏中的网址时，也应该更新通过 gtag.js 存储的网页网址。
      // https://developers.google.cn/analytics/devguides/collection/gtagjs/single-page-applications?hl=zh-cn
      

      $('.nav-main').find('.list-v').not('.menu-phone').removeAttr("style",""); // 移除小尾巴的移除
      $('.menu-phone.list-v').removeAttr("style",""); // 移除小尾巴的移除
      $('script[data-pjax], .pjax-reload script').each(function () {
        $(this).parent().append($(this).remove());
      });
      try{
          if (typeof $.fancybox == "undefined") {
            SCload_fancybox();
          } else {
            pjax_fancybox();
          }
        
        
        
        
        
        
          pjax_valine();
        
        
        
        
      } catch (e) {
        console.log(e);
      }
      window.HideLoading();
    });

    document.addEventListener('pjax:error', function (e) {
      window.HideLoading();
      window.location.href = e.triggerElement.href;
    });
</script>

    
  </div>
</body>
</html>
